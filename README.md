# ğŸš€ Project Overview

This repository provides a complete workflow for using **1D Convolutional Autoencoders** to perform unsupervised clustering and classification of time-series signals collected from physical sensors.

The practical application is to determine **if one of the known individuals is passing in front of a pair of Light-Dependent Resistors (LDRs)** and, if so, to further **identify which person it is**. During training, the autoencoders learn the unique time-series patterns generated by each known person. At test time, the system can both:

1. **Detect Movement** â€“ distinguish between background/noise (no person) and valid signals (a known person passing).
2. **Classify Identity** â€“ recognize which of the known persons produced the signal.

The project leverages a **multi-objective Pareto optimization** to balance two critical goals:

1. ğŸ“‰ **Low Reconstruction Error** â€“ The modelâ€™s ability to compress and then accurately reconstruct input signals.
2. ğŸ“ˆ **High Clustering Quality** â€“ The modelâ€™s ability to create a meaningful latent space for distinguishing between different persons and rejecting anomalies.

The workflow is divided into three automated phases: **Training**, **Evaluation**, and **Testing**.

---

## ğŸ“‚ File Structure

| File Name                 | Description                                                                                                                                        |
| ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| `data_and_train.m`        | ğŸ§  **Main Training Script**: Trains 40 different autoencoder models using a hyperparameter grid search. Generates `clustering_models_SNR_inc.mat`. |
| `test_phase1.m`           | ğŸ“Š **Evaluation & Selection Script**: Evaluates all trained models using clustering metrics and Pareto analysis to select the single best model.   |
| `test2.m`                 | ğŸ”¬ **Performance Testing Script**: Tests the best modelâ€™s classification and rejection performance on signals with varying noise levels (SNR).     |
| `daviesBouldin.m`         | ğŸ› ï¸ **Utility Function**: Calculates the Davies-Bouldin Index to evaluate clustering quality.                                                      |
| `combined_1000.mat`       | ğŸ“¦ **Input Data**: Initial dataset of signals (not included).                                                                                      |
| `clustering_models...mat` | ğŸ’¾ **Generated Models**: Contains all trained networks, created by `data_and_train.m`.                                                             |
| `test_data.mat`           | ğŸ§ª **Generated Test Data**: Contains noisy test signals, created by `test2.m`.                                                                     |

---

## âš™ï¸ Methodology Workflow

The project follows a sequential pipeline where each scriptâ€™s output becomes the next scriptâ€™s input.

### Phase 1: Model Training (`data_and_train.m`)

> In this phase, we generate a diverse set of candidate autoencoder models.

* **Data Preparation**: Loads and normalizes the time-series signals from `combined_1000.mat` (collected by the two LDRs).
* **Hyperparameter Grid**: Defines a search space for latent dimensions, filter sizes, and augmentation levels.
* **Automated Training**: Iteratively trains 40 unique 1D convolutional autoencoders.
* **Model Saving**: Exports all trained networks, encoders, and metadata to `clustering_models_SNR_inc.mat`.

---

### Phase 2: Evaluation & Selection (`test_phase1.m`)

> Here, we analyze trade-offs and select the most effective model.

* **Metric Calculation**: For each model, computes **Silhouette Score** and **Davies-Bouldin Index**.
* **Pareto Front Analysis**: Plots the trade-off between **Reconstruction Error** (lower is better) and a **Unified Cluster Score** (higher is better).
* **Best Model Selection**: Automatically picks the best compromise model closest to the ideal "Utopian Point" (zero error, perfect clustering).

![Figure\_1](https://github.com/user-attachments/assets/c4ffe386-4d71-4b85-ae88-f8aa74a4cf22)

---

### Phase 3: Performance Testing (`test2.m`)

> The final phase subjects the chosen model to stress testing in noisy, realistic conditions.

* **Noisy Data Generation**: Creates a test set by adding varying levels of Gaussian noise.
* **Practical Task**: The model checks whether the input signal corresponds to one of the known persons passing in front of the LDRs.
* **Robustness Evaluation**: Signals outside the learned latent clusters are rejected as anomalies. Valid signals are **classified by identity** (which person generated them).
* **Performance Metrics**: Computes detailed metrics such as **Macro F1-score** and **confusion matrices** at different noise levels.
* **Advanced Visualization**: Saves figures in `performance_results/`, including 3D error surfaces and confusion charts.

---

## ğŸš€ How to Run

### Prerequisites

* MATLAB (R2023b or newer recommended)
* Deep Learning Toolbox
* Statistics and Machine Learning Toolbox
* Raw data file `combined_1000.mat` in the MATLAB path

### Execution

```matlab
% 1. Train all 40 autoencoder models. This may take significant time.
>> run('data_and_train.m');

% 2. Evaluate the models and visualize the Pareto front.
>> run('test_phase1.m');

% 3. Generate noisy test data and evaluate final model performance.
>> run('test2.m');
```

---

## ğŸ“Š Outputs

The scripts generate and populate the following directories:

* `reconstruction_quality_figures/` â€“ Reconstruction quality plots for all 40 models.
* `performance_results/` â€“ Final performance plots for the best model, including 3D scatter plots, classification confusion matrices, and rejection thresholds.

---

Do you want me to also **add a small diagram/figure placeholder** in the README showing the LDR setup with â€œPerson â†’ 2 LDRs â†’ Signals â†’ Autoencoderâ€? It might make the application instantly clear for readers.
